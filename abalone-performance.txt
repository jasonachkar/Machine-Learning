----------------------------------------------------------------------------------------------------
(A) Model: Base DT - Default parameters

(B) Confusion Matrix:
          Predicted 0  Predicted 1  Predicted 2
Actual 0          139           41          170
Actual 1           46          200           67
Actual 2          133           84          165

(C) Precision, Recall, and F1-Measure for Each Class:
   Precision    Recall  F1-Score  Support
0   0.437107  0.397143  0.416168      350
1   0.615385  0.638978  0.626959      313
2   0.410448  0.431937  0.420918      382

(D) Accuracy: 0.4823
Macro-average F1: 0.4880
Weighted-average F1: 0.4810

----------------------------------------------------------------------------------------------------
(A) Model: Top DT - Best hyperparameters: {'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 6}

(B) Confusion Matrix:
          Predicted 0  Predicted 1  Predicted 2
Actual 0           97           31          222
Actual 1           42          231           40
Actual 2           75           77          230

(C) Precision, Recall, and F1-Measure for Each Class:
   Precision    Recall  F1-Score  Support
0   0.453271  0.277143  0.343972      350
1   0.681416  0.738019  0.708589      313
2   0.467480  0.602094  0.526316      382

(D) Accuracy: 0.5340
Macro-average F1: 0.5263
Weighted-average F1: 0.5198

----------------------------------------------------------------------------------------------------
(A) Model: Base MLP - hidden_layer_sizes=(100,100), activation=logistic, solver=sgd

(B) Confusion Matrix:
          Predicted 0  Predicted 1  Predicted 2
Actual 0            0           50          300
Actual 1            0          217           96
Actual 2            0           95          287

(C) Precision, Recall, and F1-Measure for Each Class:
   Precision    Recall  F1-Score  Support
0   0.000000  0.000000  0.000000      350
1   0.599448  0.693291  0.642963      313
2   0.420205  0.751309  0.538967      382

(D) Accuracy: 0.4823
Macro-average F1: 0.3940
Weighted-average F1: 0.3896

----------------------------------------------------------------------------------------------------
(A) Model: Top MLP - Best hyperparameters: {'activation': 'tanh', 'hidden_layer_sizes': (50,), 'solver': 'adam'}

(B) Confusion Matrix:
          Predicted 0  Predicted 1  Predicted 2
Actual 0           16           45          289
Actual 1            3          250           60
Actual 2           15           98          269

(C) Precision, Recall, and F1-Measure for Each Class:
   Precision    Recall  F1-Score  Support
0   0.470588  0.045714  0.083333      350
1   0.636132  0.798722  0.708215      313
2   0.435275  0.704188  0.538000      382

(D) Accuracy: 0.5120
Macro-average F1: 0.4432
Weighted-average F1: 0.4367

-------- Running all models 5 times leads to the following averages: 

(A) Average accuracy: 0.4876555023923445, variance: 2.292987797898395e-05 
(B) Average macro: 0.49287225094442755, variance: 1.936875656287355e-05 
(C) Average weighted: 0.48629882769936206, variance: 2.0668252097103683e-05 
-------- Running all models 5 times leads to the following averages: 

(A) Average accuracy: 0.5339712918660288, variance: 0.0 
(B) Average macro: 0.5262921259115242, variance: 0.0 
(C) Average weighted: 0.5198383215877659, variance: 0.0 
-------- Running all models 5 times leads to the following averages: 

(A) Average accuracy: 0.48248803827751197, variance: 1.4651679219799255e-07 
(B) Average macro: 0.3941316680979198, variance: 9.6060811961373e-08 
(C) Average weighted: 0.3897581652365527, variance: 9.903428962370085e-08 
-------- Running all models 5 times leads to the following averages: 

(A) Average accuracy: 0.5201913875598088, variance: 0.00011230512121975235 
(B) Average macro: 0.47584832517897785, variance: 0.0009961780313158274 
(C) Average weighted: 0.46889608894044804, variance: 0.0009738562618960987 
